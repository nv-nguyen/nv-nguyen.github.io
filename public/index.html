<!DOCTYPE html>
<html lang="en"><head>
	<meta name="generator" content="Hugo 0.149.1">
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  

  <title>
    
      
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.af4ea5289c8ec9e0f0023142ce5200378e27d5ada0a48b688af4b4f2480ead78d68c8f2dc3d65146596e452aafaf77a05b1445e14e58c4bfb0f72cff4a372472.css" integrity="sha512-r06lKJyOyeDwAjFCzlIAN44n1a2gpItoivS08kgOrXjWjI8tw9ZRRlluRSqvr3egWxRF4U5YxL&#43;w9yz/Sjckcg==" />
  
</head>
<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">

<header>
    <h1>Van Nguyen Nguyen</h1></header>

<img src= /images/nguyen.png style="float:right; width:40%; margin: 2.5%; border-radius: 50%;" />  

<p>I am a Research Scientist at <a href="https://www.linkedin.com/company/uii-america-inc/">United Imaging Intelligence (UII America)</a> working on 3D computer vision and robotics for clinical AI products.
Before joining UII America, I received my PhD at <a href="http://imagine.enpc.fr/">IMAGINE team</a>, <a href="http://www.enpc.fr/">√âcole des Ponts ParisTech</a>, under the supervision of <a href="https://vincentlepetit.github.io/">Prof. Vincent Lepetit</a>.</p>
<p>During my PhD, I interned twice at <a href="https://about.facebook.com/realitylabs/">Meta Reality Labs</a>, worked with <a href="https://thodan.github.io/">Tomas Hodan</a> in 2024, and <a href="https://www.linkedin.com/in/pierre-moulon/">Pierre Moulon</a> in 2022. Prior to that, I graduated from the Master <a href="https://www.master-mva.com/">Math√©matiques, Vision and Apprentissage (MVA)</a> from <a href="https://ens-paris-saclay.fr/en">√âcole Normale Sup√©rieure Paris-Saclay</a> and Engineering Program in Applied Mathematics at <a href="https://www.insa-toulouse.fr/">INSA Toulouse</a>. I also had chance to spend time at <a href="https://www.epfl.ch/labs/cvlab/">EPFL</a>, <a href="https://www.siemens.com/global/en.html">SIEMENS</a>.</p>
<p><a href="mailto:vanngn.nguyen@gmail.com">Email</a>
<a href="/download/cv_nguyen.pdf">CV</a>
<a href="https://scholar.google.com/citations?user=wctJ37UAAAAJ">Scholar</a>
<a href="https://www.linkedin.com/in/nv-nguyen/">Linkedin</a>
<a href="https://x.com/vannguyen_ng">Twitter</a>
<a href="https://github.com/nv-nguyen">Github</a></p>
<hr>
<h3 id="news">[News]</h3>
<ul>
<li>06/2025: Joined <a href="https://www.linkedin.com/company/uii-america-inc/">United Imaging Intelligence</a> in Burlington, MA as a research scientist.</li>
<li>06/2025: <a href="https://arxiv.org/pdf/2504.02812">BOP challenge 2024 report</a> received Best Paper Award at CV4MR workshop at CVPR 2025.</li>
<li>04/2024: <a href="https://arxiv.org/pdf/2504.02812">BOP challenge 2024 report</a> and <a href="https://arxiv.org/pdf/2506.07155">GoTrack</a> accepted to CV4MR workshop at CVPR 2025.</li>
<li>12/2024: PhD defended!</li>
<li>06/2024: <a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2024/">BOP challenge 2024</a> opened!</li>
<li>05/2022: Joined <a href="https://about.facebook.com/realitylabs/">Meta Reality Labs</a> as a research intern with <a href="https://thodan.github.io/">Tomas Hodan</a>.</li>
<li>04/2024: Accepted to <a href="https://cvpr.thecvf.com/Conferences/2024/CallForDoctoralConsortium">CVPR 2024 Doctoral Consortium</a>.</li>
<li>04/2024: <a href="https://arxiv.org/pdf/2403.09799.pdf">BOP challenge 2023 report</a> accepted to CVPRW 2024.</li>
<li>02/2024: <a href="https://arxiv.org/pdf/2311.14155">GigaPose</a>,  <a href="https://arxiv.org/pdf/2303.13612">NOPE</a>, <a href="https://arxiv.org/pdf/2404.18873">OSV5M</a> accepted to CVPR 2024.</li>
<li>10/2023: <a href="https://arxiv.org/abs/2307.11067">CNOS</a> accepted to R6D workshop at ICCV 2023. Awarded best 2D detection method for unseen objects at <a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2023">BOP challenge 2023</a>.</li>
<li>08/2022: Our paper <a href="https://arxiv.org/abs/2209.07589.pdf">PIZZA</a> accepted (as Oral) to 3DV 2022.</li>
<li>05/2022: Joined <a href="https://about.facebook.com/realitylabs/">Meta Reality Labs</a> as a research intern, working with <a href="https://www.linkedin.com/in/pierre-moulon/">Pierre Moulon</a>.</li>
<li>03/2022: <a href="https://arxiv.org/abs/2203.17234.pdf">Template-pose</a> accepted to CVPR 2022.</li>
<li>10/2020: Started PhD at <a href="http://imagine.enpc.fr/">IMAGINE team</a>, advised by <a href="https://vincentlepetit.github.io/">Prof. Vincent Lepetit</a>.</li>
</ul>
<hr>
<h3 id="phd-thesis-pose-estimation-of-novel-rigid-objects">[PhD thesis] Pose Estimation of Novel Rigid Objects</h3>
<p><strong>Supervisor</strong>: <a href="https://vincentlepetit.github.io/">Prof. Vincent Lepetit</a><br>
<strong>Reviewers</strong>: <a href="https://www.acin.tuwien.ac.at/en/staff/vm/">Prof. Markus Vincze</a>, <a href="https://www.cs.cit.tum.de/camp/members/benjamin-busam/">Dr. Benjamin Busam</a><br>
<strong>Examiners</strong>: <a href="https://people.ciirc.cvut.cz/~sivic/">Prof. Josef Sivic</a>, <a href="https://dimadamen.github.io/">Prof. Dima Damen</a>, <a href="https://campar.in.tum.de/Main/SlobodanIlic">Dr. Slobodan Ilic</a></p>
<ul>
<li><a href="/download/thesis.pdf">üìÑ Thesis (PDF)</a></li>
<li><a href="/download/thesis_slide.pdf">üìë Slides (PDF)</a></li>
</ul>



	<table style="border:none" cellspacing="0" cellpadding="0"><tr>	
			<td style="width:25%; border:none; vertical-align:middle;">	
    			<img src=/images/bop24.jpg style="width:100%" />
			</td>	
			<td style="width:75%; border:none; vertical-align:middle; padding:20px">	
            		<span>2025-06-11
			</span>
            		<a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2024/">BOP Challenge 2024 on Model-free, Model-based Detection, and Pose Estimation of Unseen Rigid Objects</a>
			<br>
    			<p><strong>Van Nguyen Nguyen</strong>, Stephen Tyree, Andrew Guo, M√©d√©ric Fourmy, Anas Gouda, Taeyeop Lee, Sungphill Moon, Hyeontae Son, Lukas Ranftl, Jonathan Tremblay, Eric Brachmann, Bertram Drost, Vincent Lepetit, Carsten Rother, Stan Birchfield, Jiri Matas, Yann Labb√©, Martin Sundermeyer, Tom√°≈° Hoda≈à</p>
<p>CVPRW 2025 <!-- raw HTML omitted -->(Best Paper Award)<!-- raw HTML omitted --></p>
<p>We introduce a new model-free variant of all tasks, define a new 6D object detection task, and introduce three new publicly available datasets.</p>
<p><a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2024/">[project page]</a>  <a href="https://arxiv.org/pdf/2504.02812">[arXiv]</a></p>

			</td>	
		</tr>	
        <tr>	
			<td style="width:25%; border:none; vertical-align:middle;">	
    			<img src=/images/gotrack.jpg style="width:100%" />
			</td>	
			<td style="width:75%; border:none; vertical-align:middle; padding:20px">	
            		<span>2025-06-11
			</span>
            		<a href="">GoTrack: Generic 6DoF Object Pose Refinement and Tracking</a>
			<br>
    			<p><strong>Van Nguyen Nguyen</strong>, <a href="https://www.cforster.ch">Christian Forster</a>, <a href="https://www.linkedin.com/in/sindi-shkodrani/">Sindi Shkodrani</a>, <a href="https://btekin.github.io">Bugra Tekin</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a>, <a href="https://www.linkedin.com/in/cem-keskin-23692a15">Cem Keskin</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a>, <a href="https://thodan.github.io/">Tom√°≈° Hoda≈à</a></p>
<p>CVPRW 2025</p>
<p>We introduce GoTrack, an efficient and accurate CAD-based method for 6DoF pose refinement and tracking of unseen objects. Given a CAD model of an object, an RGB image with known intrinsics that shows the object in an unknown pose, and an initial object pose, Gotrack refines the object pose such as the 2D projection of the model aligns closely with the object‚Äôs appearance in the image.</p>
<p><a href="https://arxiv.org/abs/2506.07155">[arXiv]</a>  <a href="https://github.com/facebookresearch/gotrack">[code]</a></p>

			</td>	
		</tr>	
        <tr>	
			<td style="width:25%; border:none; vertical-align:middle;">	
    			<img src=/images/bop23.png style="width:100%" />
			</td>	
			<td style="width:75%; border:none; vertical-align:middle; padding:20px">	
            		<span>2024-06-17
			</span>
            		<a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2023/">BOP Challenge 2023 on Detection, Segmentation and Pose Estimation of Seen and Unseen Rigid Objects</a>
			<br>
    			<p>Tomas Hodan, Martin Sundermeyer, Yann Labb√©, <strong>Van Nguyen Nguyen</strong>, Gu Wang, Eric Brachmann, Bertram Drost, Vincent Lepetit, Carsten Rother, Jiri Matas</p>
<p>CVPRW 2024</p>
<p>The report of BOP challenge 2023 on state-of-the-art methods for seen and unseen object pose estimation.</p>
<p><a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2023/">[project page]</a>   <a href="https://arxiv.org/pdf/2403.09799.pdf">[arXiv]</a></p>

			</td>	
		</tr>	
        <tr>	
			<td style="width:25%; border:none; vertical-align:middle;">	
    			<img src=/images/gigaPose.png style="width:100%" />
			</td>	
			<td style="width:75%; border:none; vertical-align:middle; padding:20px">	
            		<span>2024-06-17
			</span>
            		<a href="https://nv-nguyen.github.io/gigapose">GigaPose: Fast and Robust Novel Object Pose Estimation via One Correspondence</a>
			<br>
    			<p><strong>Van Nguyen Nguyen</strong>, <a href="http://imagine.enpc.fr/~groueixt/">Thibault Groueix</a>, <a href="https://people.epfl.ch/mathieu.salzmann">Mathieu Salzmann</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a></p>
<p>CVPR 2024</p>
<p>A &ldquo;hybrid&rdquo; template-patch correspondence approach that is fast, robust, and more accurate to estimate 6D pose of novel objects in RGB images. GigaPose predicts 6D object pose from a single 2D-to-2D correspondence.</p>
<p><a href="https://nv-nguyen.github.io/gigapose">[project page]</a>   <a href="http://arxiv.org/abs/2311.14155">[arXiv]</a>   <a href="https://github.com/nv-nguyen/gigapose">[code]</a></p>

			</td>	
		</tr>	
        <tr>	
			<td style="width:25%; border:none; vertical-align:middle;">	
    			<img src=/images/nope.gif style="width:100%" />
			</td>	
			<td style="width:75%; border:none; vertical-align:middle; padding:20px">	
            		<span>2024-06-17
			</span>
            		<a href="https://nv-nguyen.github.io/nope">NOPE: Novel Object Pose Estimation from a Single Image</a>
			<br>
    			<p><strong>Van Nguyen Nguyen</strong>, <a href="http://imagine.enpc.fr/~groueixt/">Thibault Groueix</a>, <a href="https://ponimatkin.github.io/">Georgy Ponimatkin</a>, <a href="https://yinlinhu.github.io/">Yinlin Hu</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a>, <a href="https://people.epfl.ch/mathieu.salzmann">Mathieu Salzmann</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a></p>
<p>CVPR 2024</p>
<p>A method that can estimate relative pose of unseen objects given only a single reference image. It also predicts 3D pose distribution which can be used to address pose ambiguities due to symmetries.</p>
<p><a href="https://nv-nguyen.github.io/nope">[project page]</a>   <a href="https://arxiv.org/abs/2303.13612">[arXiv]</a>   <a href="https://nv-nguyen.github.io/nope">[code]</a></p>

			</td>	
		</tr>	
        <tr>	
			<td style="width:25%; border:none; vertical-align:middle;">	
    			<img src=/images/osv5m.png style="width:100%" />
			</td>	
			<td style="width:75%; border:none; vertical-align:middle; padding:20px">	
            		<span>2024-06-17
			</span>
            		<a href="https://osv5m.github.io/">OpenStreetView-5M: The Many Roads to Global Visual Geolocation</a>
			<br>
    			<p>Guillaume Astruc, Nicolas Dufour, Ioannis Siglidis, Constantin Aronssohn, Nacim Bouia, Stephanie Fu, Romain Loiseau, <strong>Van Nguyen Nguyen</strong>, Charles Raude, Elliot Vincent, Lintao Xu, Hongyu Zhou, Loic Landrieu</p>
<p>CVPR 2024</p>
<p>A new benchmark for visual geolocation (~Geoguessr).</p>
<p><a href="https://osv5m.github.io/">[project page]</a>   <a href="https://arxiv.org/abs/2404.18873">[arXiv]</a>   <a href="https://github.com/gastruc/osv5m">[code]</a></p>

			</td>	
		</tr>	
        <tr>	
			<td style="width:25%; border:none; vertical-align:middle;">	
    			<img src=/images/cnos_ycb_pred.gif style="width:100%" />
			</td>	
			<td style="width:75%; border:none; vertical-align:middle; padding:20px">	
            		<span>2023-10-01
			</span>
            		<a href="https://nv-nguyen.github.io/cnos/">CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</a>
			<br>
    			<p><strong>Van Nguyen Nguyen</strong>, <a href="http://imagine.enpc.fr/~groueixt/">Thibault Groueix</a>, <a href="https://ponimatkin.github.io/">Georgy Ponimatkin</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a>, <a href="https://thodan.github.io/">Tom√°≈° Hoda≈à</a></p>
<p>ICCVW 2023 <!-- raw HTML omitted -->(Best Method Award for 2D detection of unseen objects)<!-- raw HTML omitted --></p>
<p>A method that can segment novel objects for a given RGB image from only their CAD models. Based on Segmenting Anything, DINOv2, CNOS is a strong baseline for Task 5 and 6 in the BOP challenge 2023.</p>
<p><a href="https://nv-nguyen.github.io/cnos/">[project page]</a>   <a href="https://arxiv.org/abs/2307.11067">[arXiv]</a>   <a href="https://github.com/nv-nguyen/cnos">[code]</a></p>

			</td>	
		</tr>	
        <tr>	
			<td style="width:25%; border:none; vertical-align:middle;">	
    			<img src=/images/pizza.png style="width:100%" />
			</td>	
			<td style="width:75%; border:none; vertical-align:middle; padding:20px">	
            		<span>2022-12-01
			</span>
            		<a href="https://arxiv.org/pdf/2209.07589">PIZZA: A Powerful Image-only Zero-Shot Zero-CAD Approach to 6DoF Tracking</a>
			<br>
    			<p><strong>Van Nguyen Nguyen</strong><!-- raw HTML omitted -->+<!-- raw HTML omitted -->, <a href="https://dulucas.github.io/Homepage/">Yuming Du</a><!-- raw HTML omitted -->+<!-- raw HTML omitted -->, <a href="https://youngxiao13.github.io/">Yang Xiao</a>, <a href="https://michaelramamonjisoa.github.io/">Micha√´l Ramamonjisoa</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a></p>
<p>3DV <!-- raw HTML omitted -->(Oral)<!-- raw HTML omitted --></p>
<p>A method for tracking the 6D motion of objects in RGB video sequences when neither training images nor even the 3D geometry of the objects is available.</p>
<!-- raw HTML omitted -->

			</td>	
		</tr>	
        <tr>	
			<td style="width:25%; border:none; vertical-align:middle;">	
    			<img src=/images/template-pose.png style="width:100%" />
			</td>	
			<td style="width:75%; border:none; vertical-align:middle; padding:20px">	
            		<span>2022-06-01
			</span>
            		<a href="https://nv-nguyen.github.io/template-pose">Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions</a>
			<br>
    			<p><strong>Van Nguyen Nguyen</strong>, <a href="https://yinlinhu.github.io/">Yinlin Hu</a>, <a href="https://youngxiao13.github.io/">Yang Xiao</a>, <a href="https://people.epfl.ch/mathieu.salzmann">Mathieu Salzmann</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a></p>
<p>CVPR 2022</p>
<p>A method that can recognize objects and estimate their 3D pose in color images even under partial occlusions. Our method requires neither a training phase on these objects nor real images depicting them, only their CAD models.</p>
<p><a href="https://nv-nguyen.github.io/template-pose">[project page]</a>   <a href="https://arxiv.org/abs/2203.17234">[arXiv]</a>   <a href="https://github.com/nv-nguyen/template-pose">[code]</a></p>

			</td>	
		</tr>	
        
	</table>




                
<h2 id="service">Service</h2>
<p>Reviewing:</p>
<ul>
<li>CVPR: 2022, 2023, 2024, 2025</li>
<li>ICCV: 2023, 2025</li>
<li>ECCV: 2022, 2024</li>
<li>NeurIPS: 2024, 2025</li>
<li>IROS: 2024, 2025</li>
<li>TPAMI: 2022</li>
<li>3DV: 2024</li>
<li>ACCV: 2024</li>
<li>BMVC: 2021</li>
</ul>
<p>Workshop:</p>
<ul>
<li>ECCV 2024: <a href="https://cmp.felk.cvut.cz/sixd/workshop_2024/">9th R6D workshop</a></li>
</ul>


            </div>
        </main>
    </body>
</html>
