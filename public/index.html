<!DOCTYPE html>
<html lang=""><head>
	<meta name="generator" content="Hugo 0.92.2" />
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  

  <title>
    
      
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.4/css/academicons.min.css" />
  
  
  
  <link rel="stylesheet" href="/css/main.433d83f74a27edc339f83048b85eb45476f4fbedf08fe58fe4fc516a726fad21ceadf229d96e830b9a1c1d16db27bc9c0a4bcd4f544ff8f2936e154d11ce1420.css" integrity="sha512-Qz2D90on7cM5&#43;DBIuF60VHb0&#43;&#43;3wj&#43;WP5PxRanJvrSHOrfIp2W6DC5ocHRbbJ7ycCkvNT1RP&#43;PKTbhVNEc4UIA==" />
  
</head>
<body a="light">
        <span id="theme-toggle" class="theme-toggle" aria-label="Toggle dark/light mode"></span>
        <main class="page-content" aria-label="Content">
            <div class="w">

<header style="text-align: center;">
    <h1>Van Nguyen Nguyen</h1>
    <p style="margin-top: -0.4rem; margin-bottom: 1.5rem;">Email: vanngn dot nguyen at gmail dot com</p></header>

<img src=/images/nguyen.png style="float:right; width: 180px; margin: 0 0 1.5rem 1.5rem; border-radius: 50%;" />

<p>I am a Research Scientist at <a href="https://www.linkedin.com/company/uii-america-inc/">United Imaging Intelligence (UII America)</a> in Greater Boston, MA, USA.</p>
<p>I completed my PhD at <a href="http://imagine.enpc.fr/">IMAGINE team</a>, <a href="http://www.enpc.fr/">Ã‰cole des Ponts ParisTech</a>, under the supervision of <a href="https://vincentlepetit.github.io/">Prof. Vincent Lepetit</a>.</p>
<p>My research focuses on 3D computer vision and robotics.</p>
<p><strong>Our team is always looking for self-motivated research interns. Please drop me a line if you are interested!</strong></p>
<div class="links-line" style="display: flex; font-size: 18px; gap: 1rem; justify-content: center;">
  <a href="https://scholar.google.com/citations?user=wctJ37UAAAAJ" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  <a href="https://www.linkedin.com/in/nv-nguyen/" title="LinkedIn"><i class="fa-brands fa-linkedin"></i></a>
  <a href="https://github.com/nv-nguyen" title="GitHub"><i class="fa-brands fa-github"></i></a>
  <a href="/download/cv_nguyen.pdf" title="CV"><i class="ai ai-cv"></i></a>
</div>
<hr>
<h2 id="news">News</h2>
<div id="news-container">
  <ul>
    <li>02/2026: <a href="https://arxiv.org/pdf/2512.14126">CIF</a> and <a href="https://arxiv.org/pdf/2512.06581">MedGRPO</a> accepted to CVPR 2026.</li>
    <li>01/2026: <a href="https://arxiv.org/pdf/2510.03312">Universal Beta Splatting</a> accepted to ICLR 2026.</li>
    <li>06/2025: Joined <a href="https://www.linkedin.com/company/uii-america-inc/">United Imaging Intelligence</a> in Burlington, MA as a research scientist.</li>
    <li>06/2025: <a href="https://arxiv.org/pdf/2504.02812">BOP challenge 2024 report</a> received Best Paper Award at CV4MR workshop at CVPR 2025.</li>
    <li>04/2024: <a href="https://arxiv.org/pdf/2504.02812">BOP challenge 2024 report</a> and <a href="https://arxiv.org/pdf/2506.07155">GoTrack</a> accepted to CV4MR workshop at CVPR 2025.</li>
    <li>12/2024: PhD defended!</li>
    <li>06/2024: <a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2024/">BOP challenge 2024</a> opened!</li>
    <li class="older-news" style="display: none;">05/2022: Joined <a href="https://about.facebook.com/realitylabs/">Meta Reality Labs</a> as a research intern with <a href="https://thodan.github.io/">Tomas Hodan</a>.</li>
    <li class="older-news" style="display: none;">04/2024: Accepted to <a href="https://cvpr.thecvf.com/Conferences/2024/CallForDoctoralConsortium">CVPR 2024 Doctoral Consortium</a>.</li>
    <li class="older-news" style="display: none;">04/2024: <a href="https://arxiv.org/pdf/2403.09799.pdf">BOP challenge 2023 report</a> accepted to CVPRW 2024.</li>
    <li class="older-news" style="display: none;">02/2024: <a href="https://arxiv.org/pdf/2311.14155">GigaPose</a>, <a href="https://arxiv.org/pdf/2303.13612">NOPE</a>, <a href="https://arxiv.org/pdf/2404.18873">OSV5M</a> accepted to CVPR 2024.</li>
    <li class="older-news" style="display: none;">10/2023: <a href="https://arxiv.org/abs/2307.11067">CNOS</a> accepted to R6D workshop at ICCV 2023. Awarded best 2D detection method for unseen objects at <a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2023">BOP challenge 2023</a>.</li>
    <li class="older-news" style="display: none;">08/2022: <a href="https://arxiv.org/abs/2209.07589.pdf">PIZZA</a> accepted (as Oral) to 3DV 2022.</li>
    <li class="older-news" style="display: none;">05/2022: Joined <a href="https://about.facebook.com/realitylabs/">Meta Reality Labs</a> as a research intern, working with <a href="https://www.linkedin.com/in/pierre-moulon/">Pierre Moulon</a>.</li>
    <li class="older-news" style="display: none;">03/2022: <a href="https://arxiv.org/abs/2203.17234.pdf">Template-pose</a> accepted to CVPR 2022.</li>
    <li class="older-news" style="display: none;">10/2020: Started PhD at <a href="http://imagine.enpc.fr/">IMAGINE team</a>, advised by <a href="https://vincentlepetit.github.io/">Prof. Vincent Lepetit</a>.</li>
  </ul>
<p><button id="toggle-news" onclick="toggleOlderNews()" style="margin-top: 10px; padding: 8px 16px; background: #1e70bf; color: white; border: none; border-radius: 20px; cursor: pointer; font-size: 12px; font-family: monospace; transition: background-color 0.3s ease;">Show more</button></p>
</div>
<script>
function toggleOlderNews() {
  const olderNewsItems = document.querySelectorAll('.older-news');
  const button = document.getElementById('toggle-news');
  
  if (olderNewsItems[0].style.display === 'none') {
    olderNewsItems.forEach(item => {
      item.style.display = 'list-item';
    });
    button.textContent = 'Show less';
  } else {
    olderNewsItems.forEach(item => {
      item.style.display = 'none';
    });
    button.textContent = 'Show more';
  }
}

// Add hover effect
document.addEventListener('DOMContentLoaded', function() {
  const button = document.getElementById('toggle-news');
  if (button) {
    button.addEventListener('mouseenter', function() {
      this.style.backgroundColor = '#0071bc';
    });
    button.addEventListener('mouseleave', function() {
      this.style.backgroundColor = '#1e70bf';
    });
  }
});
</script>
<hr>
<h2 id="phd-thesis-pose-estimation-of-novel-rigid-objects">PhD thesis: Pose Estimation of Novel Rigid Objects</h2>
<p><strong>Supervisor</strong>: <a href="https://vincentlepetit.github.io/">Prof. Vincent Lepetit</a><br>
<strong>Reviewers</strong>: <a href="https://www.acin.tuwien.ac.at/en/staff/vm/">Prof. Markus Vincze</a>, <a href="https://www.cs.cit.tum.de/camp/members/benjamin-busam/">Dr. Benjamin Busam</a><br>
<strong>Examiners</strong>: <a href="https://people.ciirc.cvut.cz/~sivic/">Prof. Josef Sivic</a>, <a href="https://dimadamen.github.io/">Prof. Dima Damen</a>, <a href="https://campar.in.tum.de/Main/SlobodanIlic">Dr. Slobodan Ilic</a></p>
<ul>
<li><a href="/download/thesis.pdf">ðŸ“„ Thesis (PDF)</a></li>
<li><a href="/download/thesis_slide.pdf">ðŸ“‘ Slides (PDF)</a></li>
</ul>
<hr>


<h2>Publications</h2><div class="pub-card">
		<div class="pub-img">
			<img src=/images/cif.png style="width:100%" />
		</div>
		<div class="pub-info">
			<a href="https://adreamwu.github.io/Consistent-Instance-Field/">CIF: Consistent Instance Field for Dynamic Scene Understanding</a>
			<br>
			<p><a href="https://adreamwu.github.io/">Junyi Wu</a>, <strong>Van Nguyen Nguyen</strong>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://scholar.google.com/citations?user=FdXm3_AAAAAJ">Jiachen Tao</a>, <a href="https://scholar.google.com/citations?user=5jErMqMAAAAJ">Changchang Sun</a>, <a href="https://scholar.google.com/citations?user=AFOCPkUAAAAJ">Zhongpai Gao</a>, <a href="https://scholar.google.com/citations?user=VxYsD0MAAAAJ">Zhenghao Zhao</a>, <a href="https://anwesachoudhuri.github.io/">Anwesa Choudhuri</a>, <a href="https://scholar.google.com/citations?user=0gellDsAAAAJ">Gengyu Zhang</a>, <a href="https://scholar.google.com/citations?user=TsgMRsUAAAAJ">Meng Zheng</a>, <a href="https://scholar.google.com/citations?user=7sFMIKoAAAAJ">Feiran Wang</a>, <a href="https://scholar.google.com/citations?user=PISXgwMAAAAJ">Terrence Chen</a>, <a href="https://tomyan555.github.io/">Yan Yan</a>, <a href="https://scholar.google.com/citations?user=1JDFQBUAAAAJ">Ziyan Wu</a></p>
<p>CVPR 2026</p>
<p>A continuous probabilistic spatio-temporal representation for dynamic scene understanding that disentangles visibility from persistent object identity. Our approach employs instance-embedded deformable 3D Gaussians that encode both radiance and semantic information, enabling novel-view panoptic segmentation and open-vocabulary 4D querying tasks.</p>
<span class="links-line">
  <a href="https://arxiv.org/pdf/2512.14126">[arXiv]</a>
  <a href="https://adreamwu.github.io/Consistent-Instance-Field/">[project page]</a>
</span>

		</div>
	</div>
        <div class="pub-card">
		<div class="pub-img">
			<img src=/images/medgrpo_v2.png style="width:100%" />
		</div>
		<div class="pub-info">
			<a href="https://yuhaosu.github.io/MedGRPO/">MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding</a>
			<br>
			<p><a href="https://scholar.google.com/citations?user=Q-ARFPIAAAAJ">Yuhao Su</a>, <a href="https://anwesachoudhuri.github.io/">Anwesa Choudhuri</a>, <a href="https://scholar.google.com/citations?user=AFOCPkUAAAAJ">Zhongpai Gao</a>, <a href="https://planche.me/">Benjamin Planche</a>, <strong>Van Nguyen Nguyen</strong>, <a href="https://scholar.google.com/citations?user=TsgMRsUAAAAJ">Meng Zheng</a>, <a href="https://scholar.google.com/citations?user=8vNMmGMAAAAJ">Yuhan Shen</a>, <a href="https://scholar.google.com/citations?user=7CJ-vBQAAAAJ">Arun Innanje</a>, <a href="https://scholar.google.com/citations?user=PISXgwMAAAAJ">Terrence Chen</a>, <a href="https://www.ccs.neu.edu/home/eelhami/">Ehsan Elhamifar</a>, <a href="https://scholar.google.com/citations?user=1JDFQBUAAAAJ">Ziyan Wu</a></p>
<p>CVPR 2026</p>
<p>We introduce MedVidBench, a large-scale benchmark of 531K video-instruction pairs across 8 medical sources, and MedGRPO, an RL framework with cross-dataset reward normalization and a medical LLM judge for balanced multi-dataset training. Fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash, while MedGRPO further improves grounding and captioning tasks.</p>
<span class="links-line">
  <a href="https://arxiv.org/pdf/2512.06581">[arXiv]</a>
  <a href="https://yuhaosu.github.io/MedGRPO/">[project page]</a>
</span>

		</div>
	</div>
        <div class="pub-card">
		<div class="pub-img">
			<img src=/images/ubs.png style="width:100%" />
		</div>
		<div class="pub-info">
			<a href="https://rongliu-leo.github.io/universal-beta-splatting/">Universal Beta Splatting</a>
			<br>
			<p><a href="https://rongliu-leo.github.io/">Rong Liu</a>, <a href="https://scholar.google.com/citations?user=AFOCPkUAAAAJ">Zhongpai Gao</a>, <a href="https://planche.me/">Benjamin Planche</a>, <a href="https://scholar.google.com/citations?user=ii_J7K4AAAAJ">Meida Chen</a>, <strong>Van Nguyen Nguyen</strong>, <a href="https://scholar.google.com/citations?user=TsgMRsUAAAAJ">Meng Zheng</a>, <a href="https://anwesachoudhuri.github.io/">Anwesa Choudhuri</a>, <a href="https://scholar.google.com/citations?user=PISXgwMAAAAJ">Terrence Chen</a>, <a href="https://yuewang.xyz/">Yue Wang</a>, <a href="https://ict.usc.edu/about-us/leadership/research-leadership/andrew-feng/">Andrew Feng</a>, <a href="https://scholar.google.com/citations?user=1JDFQBUAAAAJ">Ziyan Wu</a></p>
<p>ICLR 2026</p>
<p>A unified framework that generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for explicit radiance field rendering. Beta kernels naturally decompose scene properties into interpretable components (surface vs. texture, diffuse vs. specular, static vs. dynamic) without explicit supervision.</p>
<span class="links-line">
  <a href="https://arxiv.org/pdf/2510.03312">[arXiv]</a>
  <a href="https://rongliu-leo.github.io/universal-beta-splatting/">[project page]</a>
  <a href="https://github.com/RongLiu-Leo/universal-beta-splatting">[code]</a>
  <a href="https://github.com/RongLiu-Leo/universal-beta-splatting/stargazers">
    <img src="https://img.shields.io/github/stars/RongLiu-Leo/universal-beta-splatting?style=social" alt="GitHub stars">
  </a>
  <a href="https://github.com/RongLiu-Leo/universal-beta-splatting/network/members">
    <img src="https://img.shields.io/github/forks/RongLiu-Leo/universal-beta-splatting?style=social" alt="GitHub forks">
  </a>
</span>

		</div>
	</div>
        <div class="pub-card">
		<div class="pub-img">
			<img src=/images/bop24.jpg style="width:100%" />
		</div>
		<div class="pub-info">
			<a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2024/">BOP Challenge 2024 on Model-free, Model-based Detection, and Pose Estimation of Unseen Rigid Objects</a>
			<br>
			<p><strong>Van Nguyen Nguyen</strong>, Stephen Tyree, Andrew Guo, MÃ©dÃ©ric Fourmy, Anas Gouda, Taeyeop Lee, Sungphill Moon, Hyeontae Son, Lukas Ranftl, Jonathan Tremblay, Eric Brachmann, Bertram Drost, Vincent Lepetit, Carsten Rother, Stan Birchfield, Jiri Matas, Yann LabbÃ©, Martin Sundermeyer, TomÃ¡Å¡ HodaÅˆ</p>
<p>CVPRW 2025 <span class="red">(Best Paper Award)</span></p>
<p>The report of BOP Challenge 2024 on model-based and model-free 2D/6D object detection on BOP-Classic and new BOP-H3 datasets (HOT3D, HOPEv2, HANDAL).</p>
<p><a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2024/">project page</a>  <a href="https://arxiv.org/pdf/2504.02812">arXiv</a></p>

		</div>
	</div>
        <div class="pub-card">
		<div class="pub-img">
			<img src=/images/gotrack.jpg style="width:100%" />
		</div>
		<div class="pub-info">
			<a href="">GoTrack: Generic 6DoF Object Pose Refinement and Tracking</a>
			<br>
			<p><strong>Van Nguyen Nguyen</strong>, <a href="https://www.cforster.ch">Christian Forster</a>, <a href="https://www.linkedin.com/in/sindi-shkodrani/">Sindi Shkodrani</a>, <a href="https://btekin.github.io">Bugra Tekin</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a>, <a href="https://www.linkedin.com/in/cem-keskin-23692a15">Cem Keskin</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a>, <a href="https://thodan.github.io/">TomÃ¡Å¡ HodaÅˆ</a></p>
<p>CVPRW 2025</p>
<p>An efficient and accurate CAD-based method for 6DoF pose refinement and tracking of unseen objects. Given a CAD model of an object, an RGB image with known intrinsics that shows the object in an unknown pose, and an initial object pose, Gotrack refines the object pose such as the 2D projection of the model aligns closely with the objectâ€™s appearance in the image.</p>
<span class="links-line">
  <a href="https://arxiv.org/pdf/2506.07155">[arXiv]</a>
  <a href="https://github.com/facebookresearch/gotrack">[code]</a>
  <a href="https://github.com/facebookresearch/gotrack/stargazers">
    <img src="https://img.shields.io/github/stars/facebookresearch/gotrack?style=social" alt="GitHub stars">
  </a>
  <a href="https://github.com/facebookresearch/gotrack/network/members">
    <img src="https://img.shields.io/github/forks/facebookresearch/gotrack?style=social" alt="GitHub forks">
  </a>
</span>
		</div>
	</div>
        <div class="pub-card">
		<div class="pub-img">
			<img src=/images/bop23.png style="width:100%" />
		</div>
		<div class="pub-info">
			<a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2023/">BOP Challenge 2023 on Detection, Segmentation and Pose Estimation of Seen and Unseen Rigid Objects</a>
			<br>
			<p>Tomas Hodan, Martin Sundermeyer, Yann LabbÃ©, <strong>Van Nguyen Nguyen</strong>, Gu Wang, Eric Brachmann, Bertram Drost, Vincent Lepetit, Carsten Rother, Jiri Matas</p>
<p>CVPRW 2024</p>
<p>The report of BOP Challenge 2023 on state-of-the-art methods for seen and unseen object pose estimation.</p>
<p><a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2023/">project page</a>   <a href="https://arxiv.org/pdf/2403.09799.pdf">arXiv</a></p>

		</div>
	</div>
        <div class="pub-card">
		<div class="pub-img">
			<img src=/images/gigaPose.png style="width:100%" />
		</div>
		<div class="pub-info">
			<a href="https://nv-nguyen.github.io/gigapose">GigaPose: Fast and Robust Novel Object Pose Estimation via One Correspondence</a>
			<br>
			<p><strong>Van Nguyen Nguyen</strong>, <a href="http://imagine.enpc.fr/~groueixt/">Thibault Groueix</a>, <a href="https://people.epfl.ch/mathieu.salzmann">Mathieu Salzmann</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a></p>
<p>CVPR 2024</p>
<p>A &ldquo;hybrid&rdquo; template-patch correspondence approach that is fast, robust, and more accurate to estimate 6D pose of novel objects in RGB images. GigaPose predicts 6D object pose from a single 2D-to-2D correspondence.</p>
<span class="links-line">
  <a href="https://arxiv.org/pdf/2311.14155">[arXiv]</a>
  <a href="https://github.com/nv-nguyen/gigapose">[code]</a>
  <a href="https://github.com/nv-nguyen/gigapose/stargazers">
    <img src="https://img.shields.io/github/stars/nv-nguyen/gigapose?style=social" alt="GitHub stars">
  </a>
  <a href="https://github.com/nv-nguyen/gigapose/network/members">
    <img src="https://img.shields.io/github/forks/nv-nguyen/gigapose?style=social" alt="GitHub forks">
  </a>
</span>
		</div>
	</div>
        <div class="pub-card">
		<div class="pub-img">
			<img src=/images/nope.gif style="width:100%" />
		</div>
		<div class="pub-info">
			<a href="https://nv-nguyen.github.io/nope">NOPE: Novel Object Pose Estimation from a Single Image</a>
			<br>
			<p><strong>Van Nguyen Nguyen</strong>, <a href="http://imagine.enpc.fr/~groueixt/">Thibault Groueix</a>, <a href="https://ponimatkin.github.io/">Georgy Ponimatkin</a>, <a href="https://yinlinhu.github.io/">Yinlin Hu</a>, <a href="http://imagine.enpc.fr/~marletr/">Renaud Marlet</a>, <a href="https://people.epfl.ch/mathieu.salzmann">Mathieu Salzmann</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a></p>
<p>CVPR 2024</p>
<p>A method that can estimate relative pose of unseen objects given only a single reference image. It also predicts 3D pose distribution which can be used to address pose ambiguities due to symmetries.</p>
<span class="links-line">
  <a href="https://arxiv.org/pdf/2303.13612">[arXiv]</a>
  <a href="https://github.com/nv-nguyen/nope">[code]</a>
  <a href="https://github.com/nv-nguyen/nope/stargazers">
    <img src="https://img.shields.io/github/stars/nv-nguyen/nope?style=social" alt="GitHub stars">
  </a>
  <a href="https://github.com/nv-nguyen/nope/network/members">
    <img src="https://img.shields.io/github/forks/nv-nguyen/nope?style=social" alt="GitHub forks">
  </a>
</span>
		</div>
	</div>
        <div class="pub-card">
		<div class="pub-img">
			<img src=/images/osv5m.png style="width:100%" />
		</div>
		<div class="pub-info">
			<a href="https://osv5m.github.io/">OpenStreetView-5M: The Many Roads to Global Visual Geolocation</a>
			<br>
			<p>Guillaume Astruc, Nicolas Dufour, Ioannis Siglidis, Constantin Aronssohn, Nacim Bouia, Stephanie Fu, Romain Loiseau, <strong>Van Nguyen Nguyen</strong>, Charles Raude, Elliot Vincent, Lintao Xu, Hongyu Zhou, Loic Landrieu</p>
<p>CVPR 2024</p>
<p>A new benchmark for visual geolocation (~Geoguessr).</p>
<span class="links-line">
  <a href="https://arxiv.org/pdf/2404.18873">[arXiv]</a>
  <a href="https://github.com/gastruc/osv5m">[code]</a>
  <a href="https://github.com/gastruc/osv5m/stargazers">
    <img src="https://img.shields.io/github/stars/gastruc/osv5m?style=social" alt="GitHub stars">
  </a>
  <a href="https://github.com/gastruc/osv5m/network/members">
    <img src="https://img.shields.io/github/forks/gastruc/osv5m?style=social" alt="GitHub forks">
  </a>
</span>
		</div>
	</div>
        <div class="pub-card">
		<div class="pub-img">
			<img src=/images/cnos_ycb_pred.gif style="width:100%" />
		</div>
		<div class="pub-info">
			<a href="https://nv-nguyen.github.io/cnos/">CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</a>
			<br>
			<p><strong>Van Nguyen Nguyen</strong>, <a href="http://imagine.enpc.fr/~groueixt/">Thibault Groueix</a>, <a href="https://ponimatkin.github.io/">Georgy Ponimatkin</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a>, <a href="https://thodan.github.io/">TomÃ¡Å¡ HodaÅˆ</a></p>
<p>ICCVW 2023 <span class="red">(Best Method Award for 2D detection of unseen objects)</span></p>
<p>A method that can segment novel objects for a given RGB image from only their CAD models. Based on Segmenting Anything, DINOv2, CNOS is a strong baseline for Task 5 and 6 in the BOP challenge 2023.</p>
<span class="links-line">
  <a href="https://arxiv.org/pdf/2307.11067">[arXiv]</a>
  <a href="https://github.com/nv-nguyen/cnos">[code]</a>
  <a href="https://github.com/nv-nguyen/cnos/stargazers">
    <img src="https://img.shields.io/github/stars/nv-nguyen/cnos?style=social" alt="GitHub stars">
  </a>
  <a href="https://github.com/nv-nguyen/cnos/network/members">
    <img src="https://img.shields.io/github/forks/nv-nguyen/cnos?style=social" alt="GitHub forks">
  </a>
</span>
		</div>
	</div>
        <div class="pub-card">
		<div class="pub-img">
			<img src=/images/pizza.png style="width:100%" />
		</div>
		<div class="pub-info">
			<a href="https://arxiv.org/pdf/2209.07589">PIZZA: A Powerful Image-only Zero-Shot Zero-CAD Approach to 6DoF Tracking</a>
			<br>
			<p><strong>Van Nguyen Nguyen</strong><sup>+</sup>, <a href="https://dulucas.github.io/Homepage/">Yuming Du</a><sup>+</sup>, <a href="https://youngxiao13.github.io/">Yang Xiao</a>, <a href="https://michaelramamonjisoa.github.io/">MichaÃ«l Ramamonjisoa</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a></p>
<p>3DV <span class="red">(Oral)</span></p>
<p>A method for tracking the 6D motion of objects in RGB video sequences when neither training images nor even the 3D geometry of the objects is available.</p>
<span class="links-line">
  <a href="https://arxiv.org/pdf/2209.07589">[arXiv]</a>
  <a href="https://github.com/nv-nguyen/pizza">[code]</a>
  <a href="https://github.com/nv-nguyen/pizza/stargazers">
    <img src="https://img.shields.io/github/stars/nv-nguyen/pizza?style=social" alt="GitHub stars">
  </a>
  <a href="https://github.com/nv-nguyen/pizza/network/members">
    <img src="https://img.shields.io/github/forks/nv-nguyen/pizza?style=social" alt="GitHub forks">
  </a>
</span>
		</div>
	</div>
        <div class="pub-card">
		<div class="pub-img">
			<img src=/images/template-pose.png style="width:100%" />
		</div>
		<div class="pub-info">
			<a href="https://nv-nguyen.github.io/template-pose">Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions</a>
			<br>
			<p><strong>Van Nguyen Nguyen</strong>, <a href="https://yinlinhu.github.io/">Yinlin Hu</a>, <a href="https://youngxiao13.github.io/">Yang Xiao</a>, <a href="https://people.epfl.ch/mathieu.salzmann">Mathieu Salzmann</a>, <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a></p>
<p>CVPR 2022</p>
<p>A method that can recognize objects and estimate their 3D pose in color images even under partial occlusions. Our method requires neither a training phase on these objects nor real images depicting them, only their CAD models.</p>
<span class="links-line">
  <a href="https://arxiv.org/pdf/2203.17234">[arXiv]</a>
  <a href="https://github.com/nv-nguyen/template-pose">[code]</a>
  <a href="https://github.com/nv-nguyen/pizza/stargazers">
    <img src="https://img.shields.io/github/stars/nv-nguyen/template-pose?style=social" alt="GitHub stars">
  </a>
  <a href="https://github.com/nv-nguyen/template-pose/network/members">
    <img src="https://img.shields.io/github/forks/nv-nguyen/template-pose?style=social" alt="GitHub forks">
  </a>
</span>
		</div>
	</div>
        



<hr>
<h2>Industrial Experience</h2>

<div style="display: flex; justify-content: center; align-items: flex-start; gap: 30px; flex-wrap: nowrap; margin: 20px 0;">
  <div style="text-align: center; width: 140px;">
    <div style="height: 40px; display: flex; align-items: center; justify-content: center;">
      <img src="/images/united_imaging_logo.jpg" alt="United Imaging Intelligence" style="height: 35px; object-fit: contain;">
    </div>
    <p style="font-size: 12px; margin-top: 8px;">Research Scientist<br>06/2025 â€“ Present</p>
  </div>
  <div style="text-align: center; width: 140px;">
    <div style="height: 40px; display: flex; align-items: center; justify-content: center;">
      <img src="/images/meta_logo4.png" alt="Meta" style="height: 30px; object-fit: contain;">
    </div>
    <p style="font-size: 12px; margin-top: 8px;">Research Intern<br>06/2024 â€“ 02/2025<br>05/2022 â€“ 10/2022</p>
  </div>
  <div style="text-align: center; width: 140px;">
    <div style="height: 40px; display: flex; align-items: center; justify-content: center;">
      <img src="/images/siemens_logo.png" alt="Siemens" style="height: 30px; object-fit: contain;">
    </div>
    <p style="font-size: 12px; margin-top: 8px;">Research Intern<br>02/2019 â€“ 07/2019</p>
  </div>
  <div style="text-align: center; width: 140px;">
    <div style="height: 40px; display: flex; align-items: center; justify-content: center;">
      <img src="/images/neogia_icon.png" alt="Neogia" style="height: 28px; object-fit: contain;">
    </div>
    <p style="font-size: 12px; margin-top: 8px;">Research Intern<br>07/2018 â€“ 09/2018</p>
  </div>
</div>

<hr>


                
<h2 id="service">Service</h2>
<p>Reviewing: ICRA, IROS, RA-L, TPAMI, CVPR, ICCV, ECCV, NeurIPS, 3DV, ACCV, BMVC</p>
<p>Teaching: Image Processing and Artificial Vision (1st Master level) at ENPC, France in 2021, 2022</p>
<p>Workshop &amp; Challenge:</p>
<ul>
<li>R6D Workshop, <a href="https://cmp.felk.cvut.cz/sixd/workshop_2024/">9th edition at ECCV 2024</a></li>
<li>BOP Challenges: <a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2025/">2025 edition</a>, <a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2024/">2024 edition</a>, <a href="https://bop.felk.cvut.cz/challenges/bop-challenge-2023/">2023 edition</a></li>
</ul>
<p>Open source: All of my released code is maintained on my <a href="https://github.com/nv-nguyen">GitHub account</a>. <span class="links-line"><a href="https://github.com/nv-nguyen"><img src="https://img.shields.io/badge/â­_stars-1.2k-yellow?style=flat" alt="GitHub stars"></a> <a href="https://github.com/nv-nguyen"><img src="https://img.shields.io/badge/ðŸ´_forks-118-blue?style=flat" alt="GitHub forks"></a></span></p>


            </div>
        </main>
        <script>
        (function() {
          var btn = document.getElementById('theme-toggle');
          var body = document.body;

          function getEffectiveTheme() {
            var a = body.getAttribute('a');
            if (a === 'dark' || a === 'light') return a;
            return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
          }

          function updateButton() {
            btn.textContent = getEffectiveTheme() === 'dark' ? '[light]' : '[dark]';
          }

          var saved = localStorage.getItem('theme');
          if (saved) body.setAttribute('a', saved);

          updateButton();

          btn.addEventListener('click', function() {
            var next = getEffectiveTheme() === 'dark' ? 'light' : 'dark';
            body.setAttribute('a', next);
            localStorage.setItem('theme', next);
            updateButton();
          });
        })();
        </script>
    </body>
</html>
